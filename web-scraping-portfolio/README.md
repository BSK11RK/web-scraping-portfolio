# Webスクレイピング & 天気データ可視化ポートフォリオ

こんにちは。Pythonを使って Webスクレイピング や APIデータ取得、データの整理・可視化に挑戦した成果をまとめたポートフォリオです。

私はまだ Python や Webスクレイピングの初心者で、実務経験はありませんが、「まず手を動かして学ぶ」ことを大切にしています。  
未経験ながらも、新しいライブラリや手法を自分で調べて試すことに意欲的に取り組んでおり、実践的に触ってみることを重視して作っています。

このポートフォリオでは **スクレイピングと自動化をメイン** に取り組み、データ取得 → 整理 → 可視化まで一通りの流れを経験しています。  
また、HTML・CSS・JavaScript の基礎や、Flask、Django、Pygame を使った簡単なアプリ・ゲーム作成の経験もあり、現在も幅広く学習中です。

今後も新しい技術を取り入れながら、ポートフォリオの内容を継続的に充実させていく予定です。  
新しいことに挑戦し続ける姿勢を大切にしており、少しずつでもスキルを広げていきたいと考えています。

---

## 1. 技術スタック

- **Python**（文法基礎は一通り習得済み）
- **Webスクレイピング・自動化**
  - 静的ページ: `requests`, `BeautifulSoup4`
  - 動的ページ: `Selenium`, `Playwright`（実践中）
- **データ処理・可視化・保存**: `pandas`, `matplotlib`, `openpyxl`
- **フロントエンド基礎**: HTML, CSS, JavaScript
- **Webアプリ作成**: Flask, Django, Pygame, Pyxcel（簡単なアプリ・ゲーム制作経験あり）
- **CSV / JSON / Excel 出力**
- **翻訳・多言語対応（勉強中）**: `googletrans`, `DeepL`, `deep_translator`

---

## 2. 実績サンプル

### 01. Books to Scrape（静的ページ）
- 書籍のタイトル、価格、評価をまとめて取得  
- 全50ページ、1ページ20件程度を一括取得  
- CSV出力（日本円換算、日本語評価付き）

### 02. Scrapethissite（動的ページ）
- Seleniumを使ってテーブルデータを取得  
- ページネーション対応  
- CSV出力で後から分析しやすい形に整形

### 03. Open-Meteo API（天気データ）
- 緯度・経度を指定して気象データを取得  
- CSV保存、折れ線グラフ作成  
- 日本語列名・日本語ラベルに対応  
- 統計情報（平均・最高・最低気温）を表示

### 04. Books to Scrape 拡張版（多形式出力）
- 書籍タイトル（英→日翻訳）を自動取得  
- 価格を日本円に換算  
- 英語評価 → 日本語星評価に変換  
- **CSV / JSON / Excel** の3形式で保存  
- データ整形や自動化案件でも活用しやすい構成

### 05. NewsAPI（ニュース自動収集・多形式出力レポート）
- 指定キーワード（例：AI、Python、DX）に関連する最新ニュースを自動収集
- タイトル・要約・ソース名・公開日を整理して取得
- 要約テキストの長さを自動分析（平均文字数・最大文字数など）
- 最新ニュース5件のミニレポートを自動生成
- CSV / JSON / Excel の3形式で保存
- 定期実行すれば「日次ニュースレポート」として運用可能で、実務案件でも活用しやすい構成

### 06. Movie Web Scraper（映画データ自動収集・多形式出力）
- 2010～2016年の映画データを自動取得
- タイトル・公開年・評価・ジャンル・上映時間・監督・出演者・興行収入を整理して取得
- Ajax API を利用して効率的に取得（スクレイピングより安定）
- CSV / JSON / Excel の3形式で保存
- 出力データは分析・可視化・レポート作成にそのまま活用可能
- 定期実行すれば「映画データ集計レポート」として運用可能で、実務案件にも応用しやすい構成

### 07. Books To Scrape（書籍データ自動収集・多形式出力）
- 「Books to Scrape」サイトから全ページの書籍データを自動取得
- タイトル、価格（GBP）、日本円換算価格、評価（星）、詳細URL、日本語タイトルを整理して取得
- Deep Translator を使ってタイトルを日本語に自動翻訳
- 文字化け対策・エラー時リトライ・ヘッダーランダム化・プロキシ対応済みで、案件レベルの堅牢性
- CSV / JSON / Excel の3形式で保存
- 定期実行（schedule ライブラリ対応）すれば、価格推移や評価レポート作成に活用可能

### 08. Quotes To Scrape（動的ページデータ自動収集・多形式出力）
- 「Quotes to Scrape」JS版サイトから全ページの引用データを自動取得
- 名言（テキスト）と著者情報を整理して取得
- Playwright を使って JavaScript 生成コンテンツに対応
- headless モードで高速実行、ページ遷移やスクロールにも対応可能
- CSV / JSON / Excel の3形式で保存
- 動的サイトのスクレイピング案件で活用できる構成

### 09. Quotes To Scrape（英日翻訳付き・全ページ対応）
- 「Quotes to Scrape」サイトから全ページの引用データを自動取得
- 名言（英語）と日本語翻訳、著者情報を整理して取得
- 日本語翻訳部分は「。」ごとに改行を入れて見やすく表示
- CSV / JSON / Excel の3形式で保存
- JSONは整形（インデント付き）で出力
- 静的ページのスクレイピング案件で活用できる構成

### 010. Wikipedia カテゴリスクレイピング（英日翻訳付き・全記事対応）
- Wikipedia のカテゴリページから記事一覧を取得
- 各記事のタイトル・要約（最初の段落）・最終更新日を自動取得
- deep_translator を使用して「英語タイトル → 日本語タイトル」「英語要約 → 日本語要約」を自動翻訳
- 英語と日本語は 改行して読みやすく整形
- Wikipedia は CC-BY-SA のため、スクレイピング・再利用が許可されているサイト
- CSV / JSON / Excel の3形式で保存
- 文書情報の要約抽出や翻訳付きスクレイピング案件として応用可能

### 11. GitHub Trending（人気リポジトリ自動収集・多形式出力）
- GitHub の「Trending」ページから人気急上昇リポジトリを自動収集
- リポジトリ名・使用言語・スター数・URL・説明文を整理して取得
- 英語説明文と日本語翻訳（deep_translator）を取得
- 英語・日本語の説明文は「文ごとの改行」を入れて見やすく整形
- スター数の統計（平均スター数・最大スター数・人気1位リポジトリ）を自動計算
- CSV / JSON / Excel の3形式で保存
- GitHub の技術トレンド調査・日次自動レポートに活用可能
- schedule を組み合わせることで毎日自動収集にも対応可能

### 12. 気象庁データ（Selenium 自動操作 × 日平均気温取得・多形式出力）
- 気象庁「過去の気象データ検索」ページを Selenium で自動操作
- 都道府県 → 観測地点の自動クリック（東京を選択）
- 「項目を選ぶ」から 日別値 → 日平均気温 を自動選択
- 期間（開始日・終了日）を Selenium で自動入力
- 表示されたデータをテーブルから取得し、整理して pandas に格納
- CSV / JSON / Excel の3形式でデータを保存
- Matplotlib を使って 日平均気温の折れ線グラフを自動生成
- 日本語フォント設定（MS Gothic）により文字化けを完全解消
- 1か月分の気温データを日次で可視化でき、気候調査・自動レポート生成に活用可能

---

## 3. 今後挑戦したいこと・学習中

- Playwright を使った高速スクレイピング
- Excel自動レポート生成（グラフ、分析シート）
- seaborn / plotly による高度な可視化
- 翻訳API（DeepL API）との連携
- Web API × 可視化ダッシュボード
- コード品質（型ヒント、例外処理、リファクタリング）向上 

---

## 4. お問い合わせ

Webスクレイピングに関するご相談はお気軽にどうぞ。  
要件に合わせて柔軟に対応いたします。

- **Email:** nanndemo141@gmail.com  
- **GitHub:** https://github.com/BSK11RK  













